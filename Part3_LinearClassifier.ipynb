{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Part 3: Multiclass linear classification\n",
        "\u003ca id\u003dpart3\u003e\u003c/a\u003e\n",
        "$\n",
        "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
        "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
        "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
        "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
        "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
        "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
        "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
        "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
        "\\newcommand{\\pderiv}[2]{\\frac{\\partial {#1}}{\\partial {#2}}}\n",
        "$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In this part we\u0027ll learn about loss functions and how to optimize them with gradient descent.\n",
        "We\u0027ll then use this knowledge to train a very simple model: a linear SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:",
            "\n",
            "  %reload_ext",
            " ",
            "autoreload",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import torch\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport unittest\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\nplt.rcParams.update({\u0027font.size\u0027: 12})\ntorch.random.manual_seed(1904)\ntest \u003d unittest.TestCase()"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Linear Classification\n",
        "\n",
        "\u003ca id\u003dpart3_1\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In multi-class linear classification we have $C$ classes which we assume our samples\n",
        "may belong to.\n",
        "We apply a linear function to a sample $x \\in \\set{R}^{D}$ and obtain a score $s_j$ which\n",
        "represents how well $x$ fits the class $1\\leq j\\leq C$ according to our model:\n",
        "$$\n",
        "s_j \u003d \\vectr{w_j} x + b_j.\n",
        "$$\n",
        "\n",
        "Note that we have a different set of model parameters (weights) $\\vec{w_j},~b_j$ for each class,\n",
        "so a total of $C\\cdot(D+1)$ parameters.\n",
        "\n",
        "To classify a sample, we simply calculate the score for each class and choose the class with the\n",
        "highest score as our prediction.\n",
        "\n",
        "One interpretation of the weights $\\vec{w_j},~b_j$ is that they represent the parameters of an\n",
        "$N$-dimensional hyperplane. Under this interpretation the class score $s_j$ of a sample is proportional\n",
        "to the distance of that sample from the hyperplane representing the $j$-th class. Note that this score\n",
        "can be positive or negative (depending on which side of the hyperplane the sample is).\n",
        "Such a classifier therefore splits the sample space into regions where the farther a sample is from the\n",
        "positive side of a hyperplane for class $j$, the higher $s_j$, so the more likely it belongs to class $j$.\n",
        "\n",
        "![img](https://dev.datasift.com/content/1-blog/building-better-machine-learned-classifiers-faster-active-learning/700px-Hyperplane.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Implementation\n",
        "\n",
        "In the context of supervised learning of a linear classifier model, we map a dataset\n",
        "(or batch from a dataset) of $N$ samples (for example, images flattened to vectors of length $D$)\n",
        "to a score for one of each of $C$ possible classes using the linear function above.\n",
        "\n",
        "To make the implementation efficient, we\u0027ll represent the mapping with a single matrix multiplication, employing the\n",
        "\"Bias trick\": \n",
        "Instead of both $\\vec{w_j}$ and $b_j$ per class, we\u0027ll put the bias term at the end of the weight vector and\n",
        "add a term $1$ at the end of each sample.\n",
        "\n",
        "The class scores for each sample are then given by:\n",
        "\n",
        "$$\n",
        "\\mat{S} \u003d \\mat{X} \\mat{W}\n",
        "$$\n",
        "\n",
        "Where here (and in the code examples you\u0027ll work with),\n",
        "- $\\mat{X}$ is a matrix of shape $N\\times (D+1)$ containing $N$ samples in it\u0027s rows;\n",
        "- $\\mat{W}$ is of shape $(D+1)\\times C$ and contains the learnable classifier parameters (weights and bias);\n",
        "- $\\mat{S}$ is therefore a $N\\times C$ matrix of the class scores of each sample.\n",
        "\n",
        "Notes: \n",
        "1. In the following discussions we\u0027ll use the notation $\\vec{x_i}$ to denote the $i$-th training sample\n",
        "   (row $i$ in $\\mat{X}$) and $\\vec{w_j}$ to denote the weights and bias for class $j$ (column $j$ in $\\mat{W}$).\n",
        "   However, when writing explicit vectors we treat them all as columns, so e.g. $\\vectr{w_j}\\vec{x_i}$ is an\n",
        "   inner product.\n",
        "2. The reason we put the samples in the rows of $\\mat{X}$ and not columns (as is the convention in some texts) is\n",
        "   because that\u0027s the convention in the pytorch library: the batch dimension is always the first one. This has many\n",
        "   implementation advantages.\n",
        "\n",
        "**TODO** Implement a transform that performs the \"bias trick\" on a tensor in the module `hw1/transforms.py`.\n",
        "The following code will use your transform to load a subset of the [MNIST](http://yann.lecun.com/exdb/mnist/)\n",
        "dataset for us to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "ok",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# Prepare data for Linear Classifier\nimport torchvision.transforms as tvtf\nimport hw1.datasets as hw1datasets\nimport hw1.dataloaders as hw1dataloaders\nimport hw1.transforms as hw1tf\n\n# Define the transforms that should be applied to each image in the dataset before returning it\ntf_ds \u003d tvtf.Compose([\n    tvtf.ToTensor(), # Convert PIL image to pytorch Tensor\n    tvtf.Normalize(\n        # Normalize each chanel with precomputed mean and std of the train set\n        mean\u003d(0.49139968, 0.48215841, 0.44653091),\n        std\u003d(0.24703223,  0.24348513, 0.26158784)),\n    hw1tf.TensorView(-1), # Reshape to 1D Tensor\n    hw1tf.BiasTrick(), # Apply the bias trick (add bias dimension to data)\n])\n\n# Define how much data to load\nnum_train \u003d 10000\nnum_test \u003d 1000\nbatch_size \u003d 1000\n\n# Training dataset\nds_train \u003d hw1datasets.SubsetDataset(\n    torchvision.datasets.MNIST(root\u003d\u0027./data/mnist/\u0027, download\u003dTrue, train\u003dTrue, transform\u003dtf_ds),\n    num_train)\n\n# Create training \u0026 validation sets\ndl_train, dl_valid \u003d hw1dataloaders.create_train_validation_loaders(\n    ds_train, validation_ratio\u003d0.2, batch_size\u003dbatch_size\n)\n\n# Test dataset \u0026 loader\nds_test \u003d hw1datasets.SubsetDataset(\n    torchvision.datasets.MNIST(root\u003d\u0027./data/mnist/\u0027, download\u003dTrue, train\u003dFalse, transform\u003dtf_ds),\n    num_test)\ndl_test \u003d torch.utils.data.DataLoader(ds_test, batch_size)\n\nx0, y0 \u003d ds_train[0]\nn_features \u003d torch.numel(x0)\nn_classes \u003d 10\n\n# Make sure samples have bias term added\ntest.assertEqual(n_features, 28*28*1+1, \"Incorrect sample dimension\")\nprint(\"ok\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO** Complete the implementation of the `__init()__`, `predict()` and `evaluate_accuracy()` functions in the\n",
        "`LinearClassifier` class located in the `hw1/linear_classifier.py` module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Accuracy: 12.7%",
            "\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import hw1.linear_classifier as hw1linear\n",
        "\n",
        "# Create a classifier\n",
        "lin_cls \u003d hw1linear.LinearClassifier(n_features, n_classes)\n",
        "\n",
        "# Evaluate accuracy on test set\n",
        "mean_acc \u003d 0\n",
        "for (x,y) in dl_test:\n",
        "    y_pred, _ \u003d lin_cls.predict(x)\n",
        "    mean_acc +\u003d lin_cls.evaluate_accuracy(y, y_pred)\n",
        "mean_acc /\u003d len(dl_test)\n",
        "\n",
        "print(f\"Accuracy: {mean_acc:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "You should get an accuracy of around 10%, corresponding to a random guess of one of ten classes. You can run the above code block multiple times to sample different initial weights and get slightly different results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Loss Functions\n",
        "\u003ca id\u003dpart3_2\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We have seen that a linear model computes the class scores for each sample using a linear mapping as\n",
        "a score function.\n",
        "However in order to train the model, we need to define  some measure of how\n",
        "well we\u0027ve classified our samples compared to their ground truth labels.\n",
        "This measure is known as a **loss function**, and it\u0027s selection is crucial in determining the model\n",
        "that will result from training. A loss function produces lower values the better the classification is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Multiclass SVM loss function\n",
        "\n",
        "A very common linear model for classification is the Support Vector Machine. An SVM attempts to find\n",
        "separating hyperplanes that have the property of creating a maximal margin to the training samples, i.e.\n",
        "hyperplanes that are as far as possible from the closest training samples.\n",
        "For example, in the following image we see a simple case with two classes of samples that have only two features.\n",
        "The data is linearly separable and it\u0027s easy to see there are infinite possible hyperplanes (in this case lines)\n",
        "that separate the data perfectly. In this case The SVM model finds the optimal hyperplane, which is the one with\n",
        "the maximal margin. The data points closest to the separating hyperplane are called the Support Vectors\n",
        "(it can be shown that only they determine the hyperplane).\n",
        "We can see that the width of the margin is $\\frac{2}{\\norm{\\vec{w}}}$. In this simple case since the data is linearly\n",
        "separable, there exists a solution where no sample fall within the margin. If the data is not linearly separable, we\n",
        "need to allow samples to enter the margin (with a cost). This is known as a soft-margin SVM.\n",
        "\n",
        "\u003cimg src\u003d\"https://upload.wikimedia.org/wikipedia/commons/2/2a/Svm_max_sep_hyperplane_with_margin.png\" width\u003d400 alt\u003d\"svm\"/\u003e\n",
        "\n",
        "There are many ways to train an SVM model. Classically, the problem is stated as constrained optimization and\n",
        "solved with quadratic optimization techniques.\n",
        "In this exercise, we\u0027ll instead work directly with the uncontrained SVM loss function,\n",
        "calculate it\u0027s gradient analytically, and then minimize it with gradient descent.\n",
        "As we\u0027ll see in the rest of the course, this technique will be a\n",
        "major component when we train deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "\n",
        "The in-sample loss function for a multiclass soft-margin SVM can be stated as follows:\n",
        "\n",
        "$$\n",
        "L(\\mat{W}) \u003d\n",
        "\\frac{1}{N} \\sum_{i\u003d1}^{N} L_{i}(\\mat{W})\n",
        "+\n",
        "\\frac{\\lambda}{2} \\norm{\\mat{W}}^2\n",
        "$$\n",
        "\n",
        "Where the first term is the mean pointwise data-dependent loss $L_{i}$,\n",
        "given by the [hinge loss](https://en.wikipedia.org/wiki/Hinge_loss) formula,\n",
        "\n",
        "$$\n",
        "L_{i}(\\mat{W}) \u003d  \\sum_{j \\neq y_i} \\max\\left(0, \\Delta+ \\vectr{w_j} \\vec{x_i} - \\vectr{w_{y_i}} \\vec{x_i}\\right),\n",
        "$$\n",
        "\n",
        "and the second term is a regularization loss which depends only on model parameters.\n",
        "Note that the hinge loss term sums over the *wrong* class prediction scores for each sample.\n",
        "This can be understood as attempting to make sure that the score for the correct class is higher than the other \n",
        "classes by\n",
        "at least some margin $\\Delta \u003e 0$, otherwise a loss is incurred. Since this is a soft-margin SVM we allow samples\n",
        "to fall within the margin but it accumulates loss.\n",
        "The regularization term penalizes large weight magnitudes to prevent ambiguous solutions since if \n",
        "e.g. $\\mat{W^*}$ is a weight matrix that perfectly separates the data, so is $\\alpha\\mat{W^*}$ for\n",
        "any scalar $\\alpha \\geq 1$.\n",
        "\n",
        "Fitting an SVM model then amounts to finding the weight matrix $\\mat{W}$ which minimizes $L(\\mat{W})$.\n",
        "Note that we\u0027re writing the loss as a function of $\\mat{W}$ to\n",
        "emphasize that we wish to minimize it\u0027s value on the given data by with respect to the weights $\\mat{W}$,\n",
        "even though it obviously depends also on the specific dataset, $\\left\\{ \\vec{x_i}, y_i \\right\\}_{i\u003d1}^{N}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Implementation\n",
        "\n",
        "**TODO** Implement the SVM hinge loss function in the module `hw1/losses.py`, within the `SVMHingeLoss` class.\n",
        "Implement just the `loss()` function. For now you can ignore the part about saving tensors for the gradient calculation. Run the following to test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [1., 1., 1.],\n",
            "        [2., 2., 2.]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[autoreload of hw1.losses failed: Traceback (most recent call last):\n",
            "  File \"C:\\Anaconda3\\envs\\cs236605-hw\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 244, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "  File \"C:\\Anaconda3\\envs\\cs236605-hw\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 378, in superreload\n",
            "    module \u003d reload(module)\n",
            "  File \"C:\\Anaconda3\\envs\\cs236605-hw\\lib\\imp.py\", line 314, in reload\n",
            "    return importlib.reload(module)\n",
            "  File \"C:\\Anaconda3\\envs\\cs236605-hw\\lib\\importlib\\__init__.py\", line 169, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"\u003cfrozen importlib._bootstrap\u003e\", line 630, in _exec\n",
            "  File \"\u003cfrozen importlib._bootstrap_external\u003e\", line 728, in exec_module\n",
            "  File \"\u003cfrozen importlib._bootstrap\u003e\", line 219, in _call_with_frames_removed\n",
            "  File \"C:\\Users\\דידי\\PycharmProjects\\DL1\\hw1\\losses.py\", line 92, in \u003cmodule\u003e\n",
            "    s_yi \u003d torch.gather(input\u003dx_scores, dim\u003d1, index\u003dy_m)\n",
            "RuntimeError: Expected object of scalar type Long but got scalar type Float for argument #3 \u0027index\u0027\n",
            "]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "output with shape [1, 28, 28] doesn\u0027t match the broadcast shape [3, 28, 28]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-14-29bd86e1817d\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Classify all samples in the test set (because it doesn\u0027t depend on initialization)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 8\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mdl_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_scores\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mlin_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\PycharmProjects\\DL1\\cs236605\\dataloader_utils.py\u001b[0m in \u001b[0;36mflatten\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mout_tensors_cache\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 23\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Handle case of batch being a tensor (no labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\cs236605-hw\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m\u003d\u003d\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\cs236605-hw\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m\u003clistcomp\u003e\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[1;33m\u003d\u003d\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# same-process loading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 615\u001b[1;33m             \u001b[0mbatch\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\PycharmProjects\\DL1\\hw1\\datasets.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m\u003e\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubset_len\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m\u003c\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\u0027\u0027\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 66\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\cs236605-hw\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\cs236605-hw\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 60\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\cs236605-hw\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \"\"\"\n\u001b[1;32m--\u003e 163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Anaconda3\\envs\\cs236605-hw\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[0mmean\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mstd\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 208\u001b[1;33m     \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: output with shape [1, 28, 28] doesn\u0027t match the broadcast shape [3, 28, 28]"
          ]
        }
      ],
      "source": [
        "import cs236605.dataloader_utils as dl_utils\n",
        "from hw1.losses import SVMHingeLoss\n",
        "\n",
        "# Create a hinge-loss function\n",
        "loss_fn \u003d SVMHingeLoss(delta\u003d1.)\n",
        "\n",
        "# Classify all samples in the test set (because it doesn\u0027t depend on initialization)\n",
        "x, y \u003d dl_utils.flatten(dl_test)\n",
        "y_pred, x_scores \u003d lin_cls.predict(x)\n",
        "\n",
        "# Compute loss\n",
        "loss \u003d loss_fn(x, y, x_scores, y_pred)\n",
        "\n",
        "# Compare to pre-computed expected value as a test\n",
        "expected_loss \u003d 8.9579\n",
        "print(\"loss \u003d\", loss.item())\n",
        "print(\u0027diff \u003d\u0027, abs(loss.item()-expected_loss))\n",
        "test.assertAlmostEqual(loss.item(), expected_loss, delta\u003d1e-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Optimizing a Loss Function with Gradient Descent\n",
        "\u003ca id\u003dpart3_3\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In this section we\u0027ll implement a simple gradient descent optimizer for the loss function we\u0027ve implemented above. As you may recall from the lectures, the basic gradient-based optimization scheme is as follows:\n",
        "\n",
        "1. Start with initial model weights $\\mat{W_0}$ initialized randomly.\n",
        "1. For $k\u003d1,2,\\dots,K$:\n",
        "    1. Select a step size $\\eta_k$.\n",
        "    1. Compute the gradient of the loss w.r.t. $\\mat{W}$ and evaluate at the current weights:\n",
        "        $\\nabla_{\\mat{W}} L(\\mat{W_{k-1}})$.\n",
        "    1. Update: \n",
        "        $$\n",
        "        \\mat{W_k} \u003d \\mat{W_{k-1}} - \\eta_k \\nabla_{\\mat{W}} L(\\mat{W_{k-1}})\n",
        "        $$\n",
        "    1. Stop if minimum reached or validation-set loss is low enough.\n",
        "\n",
        "The crucial component here is the gradient calculation. In this exercise we\u0027ll analytically derive the gradient\n",
        "of the loss and then implement it in code.\n",
        "\n",
        "An important detail to note is that while $L(\\mat{W})$ is scalar-valued, it\u0027s a function of all the elements of the\n",
        "matrix $\\mat{W}$. Therefore it\u0027s gradient w.r.t. $\\mat{W}$ is also a matrix of the same shape as $\\mat{W}$:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mat{W}} L \u003d\n",
        "\\begin{bmatrix}\n",
        "    \\frac{\\partial L}{\\partial W_{1,1}} \u0026 \u0026 \\cdots \u0026 \\frac{\\partial L}{\\partial W_{1,C}} \\\\\n",
        "    \\frac{\\partial L}{\\partial W_{2,1}} \u0026 \\ddots \u0026  \\\\\n",
        "    \\vdots \u0026 \u0026 \\ddots \u0026  \\\\\n",
        "    \\frac{\\partial L}{\\partial W_{D,1}} \u0026 \\cdots \u0026  \u0026 \\frac{\\partial L}{\\partial W_{D,C}} \\\\\n",
        "\\end{bmatrix} \u003d\n",
        "\\begin{bmatrix}\n",
        "\\vert \u0026 \u0026 \\vert \\\\\n",
        "\\frac{\\partial L}{\\partial\\vec{w_1}} \u0026 \\cdots \u0026 \\frac{\\partial L}{\\partial\\vec{w_C}}\\\\\n",
        "\\vert \u0026 \u0026 \\vert \\\\\n",
        "\\end{bmatrix}\n",
        "\\in \\set{R}^{(D+1)\\times C}.\n",
        "$$\n",
        "\n",
        "For our gradient descent update-step we\u0027ll need to create such a matrix of derivatives and evaluate it at the \n",
        "current value of the weight matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### SVM loss gradient\n",
        "\n",
        "The first thing we need to do is formulate an expression for the gradient of the loss function defined above. Since the expression for the loss depends on the columns of $\\mat{W}$, we\u0027ll derive an expression for the gradient of $L(\\mat{W})$ w.r.t. each $\\vec{w_j}$:\n",
        "\n",
        "$$\n",
        "\\pderiv{L}{\\vec{w_j}}(\\mat{W}) \u003d \n",
        "\\frac{1}{N} \\sum_{i\u003d1}^{N} \\pderiv{L_{i}}{\\vec{w_j}}(\\mat{W})\n",
        "+\n",
        "\\lambda \\mat{W}.\n",
        "$$\n",
        "\n",
        "To compute the gradient of the pointwise loss, let\u0027s define the **margin-loss** of sample $i$ for class $j$\n",
        "as follows: $m_{i,j} \u003d \\Delta + \\vectr{w_j}\\vec{x_i} - \\vectr{w_{y_i}}\\vec{x_i}$.\n",
        "We can then write the pointwise loss and it\u0027s gradient in terms of $m_{i,j}$. We\u0027ll separate the case of $j\u003dy_i$\n",
        "(i.e. the gradient for the correct class):\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\pderiv{L_i}{\\vec{w_j}} \u0026 \u003d\n",
        "        \\begin{cases}\n",
        "            \\vec{x_i}, \u0026 m_{i,j}\u003e0 \\\\\n",
        "            0, \u0026 \\mathrm{else} \\\\\n",
        "        \\end{cases}\n",
        "    ,~j \\neq y_i \\\\\n",
        "    \\\\\n",
        "    \\pderiv{L_i}{\\vec{w_{y_i}}} \u0026 \u003d -\\vec{x_i} \\sum_{j\\neq y_i} \\mathbb{1}\\left( m_{i,j} \u003e 0 \\right)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Where $\\mathbb{1}(\\cdot)$ is an indicator function that takes the value $1$ if it\u0027s argument is a true statement, else it takes $0$.\n",
        "\n",
        "Note: the hinge-loss function is not strictly speaking differentiable due to the $\\max$ operator.\n",
        "However, in practice it\u0027s not a major concern. Given that we know what argument the $\\max$ \"cooses\",\n",
        "we can differentiate each one of them separately. This is known as a sub-gradient. In the above, when $m_{i,j} \\leq 0$ we know the gradient will simply be zero.\n",
        "\n",
        "**TODO** Based on the above, implement the gradient of the loss function in the module `hw1/losses.py`,\n",
        "within the `SVMHingeLoss` class.\n",
        "Implement the `grad()` function and complete what\u0027s missing in the `loss()` function.\n",
        "Make sure you understand the above gradient derivation before attempting to implement it.\n",
        "\n",
        "Note: you\u0027ll be implementing the first term in the above equation for $\\pderiv{L}{\\vec{w_j}}(\\mat{W})$. We\u0027ll add the regularization term later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from hw1.losses import SVMHingeLoss\n",
        "\n",
        "# Create a hinge-loss function\n",
        "loss_fn \u003d SVMHingeLoss(delta\u003d1)\n",
        "\n",
        "# Compute loss and gradient\n",
        "loss \u003d loss_fn(x, y, x_scores, y_pred)\n",
        "grad \u003d loss_fn.grad()\n",
        "\n",
        "# Test the gradient with a pre-computed expected value\n",
        "expected_grad \u003d torch.load(\u0027tests/assets/part3_expected_grad.pt\u0027)\n",
        "diff \u003d torch.norm(grad - expected_grad)\n",
        "print(\u0027diff \u003d\u0027, diff.item())\n",
        "test.assertAlmostEqual(diff, 0, delta\u003d1e-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Training the model with SGD\n",
        "\u003ca id\u003dpart3_4\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now that we have implemented our loss function and it\u0027s gradient, we can finally train our model.\n",
        "Generally, solving a machine-learning problem requires defining the following components:\n",
        "- A model:\n",
        "  architecture (type of model) consisting of hyperparameters (e.g. number of hidden layers, number of classes, etc)\n",
        "  which are set in advance and trainable parameters which we want to fit to data.\n",
        "- A loss function (sometimes denoted as a criterion):\n",
        "  evaluates the model output on some data compared to ground truth.\n",
        "- An optimization scheme:\n",
        "  specifies how the model should be updated to improve the loss. May also have hyperparameters.\n",
        "- A dataset:\n",
        "  What to fit the model to. Usually the available data is split into training, validation and test sets.\n",
        "\n",
        "\n",
        "Implementation notes:\n",
        "- You\u0027ll find that when implementing your solutions it\u0027s wise to keep the above components separate as to be\n",
        "  able to change each one of them independently from the other.\n",
        "- In this exercise we\u0027ll have separated the loss and dataset, however for simplicity we\u0027ll implement the\n",
        "  model and optimizer together.\n",
        "- As you\u0027ll see further on, `PyTorch` provides very effective mechanisms to implement all of\n",
        "  these components in a decoupled manner.\n",
        "- Note that our loss implementation didn\u0027t include regularization. We\u0027ll add this during the training phase\n",
        "  using the `weight_decay` parameter. The reason is that we prefer that the part of the loss which only depends\n",
        "  on the model parameters be part of the optimizer, not the loss function (though both ways are possible).\n",
        "  You\u0027ll see this pattern later on when you use `PyTorch`\u0027s optimizers in the `torch.optim` package.\n",
        "- In practice we use batches of samples from the training set when training the model, because usually the training\n",
        "  set can\u0027t fit into memory. Using gradients computed on batches of data at a time is known as mini-batch\n",
        "  stochastic gradient descent (SGD).\n",
        "\n",
        "**TODO** Implement the model training loop in the `LinearClassifier`\u0027s `train()` function.\n",
        "Use mini-batch SGD for the weight update rule.\n",
        "\n",
        "Note: You should play with the hyperparameters to get a feel for what they do to the loss and accuracy graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "lin_cls \u003d hw1linear.LinearClassifier(n_features, n_classes)\n",
        "\n",
        "# Evaluate on the test set\n",
        "x_test, y_test \u003d dl_utils.flatten(dl_test)\n",
        "y_test_pred , _\u003d lin_cls.predict(x_test)\n",
        "test_acc_before \u003d lin_cls.evaluate_accuracy(y_test, y_test_pred)\n",
        "\n",
        "# Train the model\n",
        "svm_loss_fn \u003d SVMHingeLoss()\n",
        "train_res, valid_res \u003d lin_cls.train(dl_train, dl_valid, svm_loss_fn,\n",
        "                                    learn_rate\u003d1e-3, weight_decay\u003d0.5,\n",
        "                                    max_epochs\u003d31)\n",
        "\n",
        "# Re-evaluate on the test set\n",
        "y_test_pred , _\u003d lin_cls.predict(x_test)\n",
        "test_acc_after \u003d lin_cls.evaluate_accuracy(y_test, y_test_pred)\n",
        "\n",
        "# Plot loss and accuracy\n",
        "fig, axes \u003d plt.subplots(nrows\u003d1, ncols\u003d2, figsize\u003d(10,5))\n",
        "for i, loss_acc in enumerate((\u0027loss\u0027, \u0027accuracy\u0027)):\n",
        "    axes[i].plot(getattr(train_res, loss_acc))\n",
        "    axes[i].plot(getattr(valid_res, loss_acc))\n",
        "    axes[i].set_title(loss_acc.capitalize(), fontweight\u003d\u0027bold\u0027)\n",
        "    axes[i].set_xlabel(\u0027Epoch\u0027)\n",
        "    axes[i].legend((\u0027train\u0027, \u0027valid\u0027))\n",
        "    axes[i].grid(which\u003d\u0027both\u0027, axis\u003d\u0027y\u0027)\n",
        "    \n",
        "# Check test set accuracy\n",
        "print(f\u0027Test-set accuracy before training: {test_acc_before:.1f}%\u0027)\n",
        "print(f\u0027Test-set accuracy after training: {test_acc_after:.1f}%\u0027)\n",
        "test.assertGreaterEqual(test_acc_after, 80.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Even though this is a very naïve model, you should get at least 80% test set accuracy if you implemented training correctly. You can try to change the hyperparameters and see whether you get better results. Generally this should be done with cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "One way to unnderstand what models learn is to try to visualize their learned parameters.\n",
        "There can be many ways to do this. Let\u0027s try a very simple one, which is to reshape them into images of the input\n",
        "size and see what they look like.\n",
        "\n",
        "**TODO** Implement the `weights_as_images()` function in the `LinearClassifier` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import cs236605.plot as plot\n",
        "\n",
        "w_images \u003d lin_cls.weights_as_images(img_shape\u003d(1,28,28))\n",
        "fig, axes \u003d plot.tensors_as_images(list(w_images))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Additionally, we can better understand the model by plotting some samples and looking at wrong predictions.\n",
        "Run the following block to visualize some test-set examples and the model\u0027s predictions for them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Plot some images from the test set and their predictions\n",
        "n_plot \u003d 104\n",
        "x_test, y_test \u003d next(iter(dl_test))\n",
        "x_test \u003d x_test[0:n_plot]\n",
        "y_test \u003d y_test[0:n_plot]\n",
        "y_test_pred, _ \u003d lin_cls.predict(x_test)\n",
        "x_test_img \u003d torch.reshape(x_test[:, :-1], (n_plot, 1, 28, 28))\n",
        "\n",
        "fig, axes \u003d plot.tensors_as_images(list(x_test_img), titles\u003dy_test_pred.numpy(),\n",
        "                                   nrows\u003d8, hspace\u003d0.5, figsize\u003d(10,8), cmap\u003d\u0027gray\u0027)\n",
        "\n",
        "# Highlight the wrong predictions\n",
        "wrong_pred \u003d y_test_pred !\u003d y_test\n",
        "wrong_pred_axes \u003d axes.ravel()[wrong_pred.numpy().astype(np.bool)]\n",
        "for ax in wrong_pred_axes:\n",
        "    ax.title.set_color(\u0027red\u0027)\n",
        "    ax.title.set_fontweight(\u0027bold\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Automatic differentiation\n",
        "\u003ca id\u003dpart3_5\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In the simple linear model we worked with, the gradient was fairly straightforward to derive analytically\n",
        "and implement.\n",
        "However for complex models such as deep neural networks with many layers and non-linear operations between\n",
        "them this is not the case. Additionally, the gradient must be re-derived any time either the model\n",
        "architecture or the loss function changes. These things make it infeasible in practice to perform\n",
        "deep-learning research using this manual method of gradient derivation.\n",
        "Therefore, all deep-learning frameworks provide a mechanism of [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), to prevent\n",
        "the user from needing to manually derive the gradients of loss functions.\n",
        "\n",
        "`PyTorch` provides this functionality in a package named `torch.autograd` which we will use further on in the\n",
        "next exercises.\n",
        "For now, here\u0027s an example showing that autograd can compute the gradient of the loss function you\u0027ve implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Create a new classifier\n",
        "lin_cls \u003d hw1linear.LinearClassifier(n_features, n_classes)\n",
        "x, y \u003d dl_utils.flatten(dl_test)\n",
        "\n",
        "# Specify we want the gradient to be saved for the weights tensor\n",
        "lin_cls.weights.requires_grad \u003d True\n",
        "\n",
        "# Forward pass using the weights tensor, operations will be tracked\n",
        "y_pred, x_scores \u003d lin_cls.predict(x)\n",
        "\n",
        "# Compute loss and analytic gradient\n",
        "loss_fn \u003d SVMHingeLoss(delta\u003d1)\n",
        "loss \u003d loss_fn(x, y, x_scores, y_pred)\n",
        "grad \u003d loss_fn.grad()\n",
        "\n",
        "# Compute gradient with autograd\n",
        "loss.backward()\n",
        "autograd \u003d lin_cls.weights.grad\n",
        "\n",
        "diff \u003d torch.norm(grad - autograd).item()\n",
        "print(\u0027loss \u003d\u0027, loss.item())\n",
        "print(\u0027grad \u003d\\n\u0027, grad)\n",
        "print(\u0027autograd \u003d\\n\u0027, autograd)\n",
        "print(\u0027diff \u003d\u0027, diff)\n",
        "test.assertLess(diff, 1e-3, \"Gradient diff was too large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Questions\n",
        "\n",
        "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw1/answers.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from cs236605.answers import display_answer\n",
        "import hw1.answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Question 1 \n",
        "\n",
        "Explain why the selection of $\\Delta \u003e 0$ is arbitrary for the SVM loss $L(\\mat{W})$ as it is defined above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display_answer(hw1.answers.part3_q1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Question 2\n",
        "\n",
        "Given the images in the visualization section above,\n",
        "\n",
        "1. How do you interpret what the linear model is actually learning? Can you explain some of the classification\n",
        "   errors based on it?\n",
        "1. How is this interpretation similar or different from KNN?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display_answer(hw1.answers.part3_q2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Question 3\n",
        "\n",
        "1. Based on the graph of the training set loss, would you say that the learning rate is:\n",
        "    - Too low\n",
        "    - Good\n",
        "    - Too High\n",
        "    \n",
        "  Explain your answer by describing what the loss graph would look like in the other two cases when training\n",
        "  for the same number of epochs.\n",
        "  \n",
        "1. Based on the graph of the training and test set accuracy, would you say that the model is:\n",
        "    - Slightly overfitted to the training set\n",
        "    - Highly overfitted to the training set\n",
        "    - Slightly underfitted to the training set\n",
        "    - Highly underfitted to the training set\n",
        "    \n",
        "  and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display_answer(hw1.answers.part3_q3)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}