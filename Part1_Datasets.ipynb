{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In this part, we\u0027ll learn about the `Dataset` and `DataLoader` classes which are part of `PyTorch`\u0027s `torch.util.data` package.\n",
        "These are highly useful abstractions that can greatly reduce the amount of boilerplate code you need to write in order to work with data.\n",
        "Knowing how to use these classes properly will prove useful in the coming assignments and course project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import unittest\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "plt.rcParams.update({\u0027font.size\u0027: 12})\n",
        "torch.random.manual_seed(1904)\n",
        "test \u003d unittest.TestCase()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Datasets\n",
        "\u003ca id\u003dpart1_1\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The `Dataset` class is an abstraction over a sequence of python objects,\n",
        "each representing a sample (with or without a label). it\u0027s main purpose is\n",
        "to load a single (possibly labeled) sample from some soure (disk, web, etc) into memory,\n",
        "and transform it into a usuable representation (e.g. image to tensor).\n",
        "\n",
        "The `Dataset` abstracts away exactly when the data is loaded into memory: It can be on\n",
        "demand when each sample is accessed, all in advance or some combination using e.g. caching.\n",
        "This is implementation-specific.\n",
        "\n",
        "Lets create a demonstration `Dataset` that returns noise images. It should:\n",
        "- Return random tensors of size `CxWxH`.\n",
        "- Label each returned tensor with a class label, an integer between `0` and `num_classes-1`.\n",
        "- Initialize each returned tensor with a uniform distribution on `[0, 255]`.\n",
        "- Return a total of `num_samples` labeled images.\n",
        "\n",
        "**TODO** Implement the `RandomImageDataset` class in the `hw1/datasets.py` module.\n",
        "Use the code below to test your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-9-416683cbc1fe\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Create the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 8\u001b[1;33m \u001b[0mnum_samples\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m\u003cipython-input-9-416683cbc1fe\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Create the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 8\u001b[1;33m \u001b[0mnum_samples\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mimage_size\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_37_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_37_64.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_37_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_37_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_37_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_37_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_37_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_37_64.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32m_pydevd_bundle\\pydevd_cython_win32_37_64.pyx\u001b[0m in \u001b[0;36m_pydevd_bundle.pydevd_cython_win32_37_64.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32mC:\\Program Files\\JetBrains\\PyCharm 2019.1\\helpers\\pydev\\pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, send_suspend_message)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 877\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_thread_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "source": [
        "# Test RandomImageDataset\n",
        "\n",
        "import itertools\n",
        "import cs236605.plot as plot\n",
        "import hw1.datasets as hw1datasets\n",
        "\n",
        "# Create the dataset\n",
        "num_samples \u003d 1000\n",
        "num_classes \u003d 10\n",
        "image_size \u003d (3, 32, 32)\n",
        "ds \u003d hw1datasets.RandomImageDataset(num_samples, num_classes, *image_size)\n",
        "\n",
        "# You can load individual items from the dataset by indexing\n",
        "img0, cls0 \u003d ds[0]\n",
        "\n",
        "# Plot first N images from the dataset with a helper function\n",
        "fig, axes \u003d plot.dataset_first_n(ds, 9, show_classes\u003dTrue, nrows\u003d3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Built-in Datasets and Transforms\n",
        "\u003ca id\u003dpart1_2\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now that we\u0027ve created a simple `Dataset` to see how they work, we\u0027ll load one of `pytorch`\u0027s built-in datasets: CIFAR-10. This is a famous dataset consisting of 60,000 small `32x32` images classified into 10 classes. You can read more about it [here](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "The `torchvision` package has built-in `Dataset` classes that can download the data to a local folder,\n",
        "load it, transform it using arbitrary transform functions and iterate over the resulting samples.\n",
        "\n",
        "Run the following code block to download and create a CIFAR-10 `Dataset`. It won\u0027t be downloaded again if already present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "numpy.core.multiarray failed to import",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-1-f556c6bfc4db\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[1;32m----\u003e 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtvtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcfar10_labels\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\u0027plane\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\u0027car\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\u0027bird\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\u0027cat\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\u0027deer\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\u0027dog\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\u0027frog\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\u0027horse\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\u0027ship\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\u0027truck\u0027\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs236605-hw\\lib\\site-packages\\torchvision\\__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[1;32m----\u003e 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs236605-hw\\lib\\site-packages\\torchvision\\models\\__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[1;32m----\u003e 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0malexnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvgg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0msqueezenet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minception\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs236605-hw\\lib\\site-packages\\torchvision\\models\\alexnet.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[1;32m----\u003e 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_zoo\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodel_zoo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\u0027AlexNet\u0027\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\u0027alexnet\u0027\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\cs236605-hw\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 102\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m __all__ +\u003d [name for name in dir(_C)\n",
            "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as tvtf\n",
        "\n",
        "cfar10_labels \u003d (\u0027plane\u0027, \u0027car\u0027, \u0027bird\u0027, \u0027cat\u0027, \u0027deer\u0027, \u0027dog\u0027, \u0027frog\u0027, \u0027horse\u0027, \u0027ship\u0027, \u0027truck\u0027)\n",
        "\n",
        "cifar10_train_ds \u003d torchvision.datasets.CIFAR10(\n",
        "    root\u003d\u0027./data/cifar-10/\u0027, download\u003dTrue, train\u003dTrue,\n",
        "    transform\u003dtvtf.ToTensor() # Convert PIL image to pytorch Tensor\n",
        ")\n",
        "\n",
        "print(\u0027Number of samples:\u0027, len(cifar10_train_ds))\n",
        "\n",
        "# Plot them with a helper function\n",
        "fig, axes \u003d plot.dataset_first_n(cifar10_train_ds, 64,\n",
        "                                 show_classes\u003dTrue, class_labels\u003dcfar10_labels,\n",
        "                                 nrows\u003d8, hspace\u003d0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now that we\u0027ve loaded the entire CIFAR-10 dataset, we would want to work with a smaller subset\n",
        "from it to reduce runtime of the code in this notebook.\n",
        "A simple way to achieve this with `Datasets` is to wrap a `Dataset` in another `Dataset` that does this for us. This will make it easy to use our subset with `DataLoader`s as you will see later.\n",
        "\n",
        "**TODO** Complete the implementation of `SubsetDataset` in `hw1/datasets.py` and use the following code block to test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "subset_len \u003d 5000\n",
        "subset_offset \u003d 1234\n",
        "cifar10_train_subset_ds \u003d hw1datasets.SubsetDataset(cifar10_train_ds, subset_len, subset_offset)\n",
        "\n",
        "dataset_x, dataset_y  \u003d cifar10_train_ds[subset_offset + 10]\n",
        "subset_x, subset_y  \u003d cifar10_train_subset_ds[10]\n",
        "\n",
        "# Tests\n",
        "test.assertEqual(len(cifar10_train_subset_ds), subset_len)\n",
        "test.assertTrue(torch.all(dataset_x \u003d\u003d subset_x))\n",
        "test.assertEqual(dataset_y, subset_y)\n",
        "with test.assertRaises(IndexError, msg\u003d\"Out of bounds index should raise IndexError\"):\n",
        "    tmp \u003d cifar10_train_subset_ds[subset_len]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### `DataLoader`s and `Sampler`s\n",
        "\u003ca id\u003dpart1_3\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We have seen that a `Dataset` is simply an iterable returning samples by index.\n",
        "Simple to implement, but not very powerful.\n",
        "The real benefit is when combining them with `DataLoader`.\n",
        "A `DataLoader` samples a batch of samples from the dataset according to logic defined by a `Sampler` object.\n",
        "The sampler decides how to partition the dataset into batches of `N` samples.\n",
        "The `DataLoader` additionally handles loading samples in parallel to speed up creation of a batch.\n",
        "\n",
        "A major motivation here is memory usage. When combining a `DataLoader` with a `Dataset` we can easily\n",
        "control memory constraints by simple setting the batch size. This is important since large\n",
        "datasets (e.g. ImageNet) may not fit in memory of most machines.\n",
        "Since a `Dataset` can lazily load samples from disk on access,\n",
        "and the `DataLoader` can sample random samples from it in parallel, we are provided with a simple\n",
        "yet high-performance mechanism to iterate over random batches from our dataset without needing to\n",
        "hold all of it in memory.\n",
        "\n",
        "Let\u0027s create a basic `DataLoader` for our CIFAR-10 dataset.\n",
        "Run the follwing code block multiple times and observe that different samples are shown each time in the first few batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-5-37ef1755fc32\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create a simple DataLoader that partitions the data into batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# of size N\u003d8 in random order, using two background proceses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 3\u001b[1;33m cifar10_train_dl \u003d torch.utils.data.DataLoader(\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mcifar10_train_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name \u0027torch\u0027 is not defined"
          ],
          "ename": "NameError",
          "evalue": "name \u0027torch\u0027 is not defined",
          "output_type": "error"
        }
      ],
      "source": [
        "# Create a simple DataLoader that partitions the data into batches\n",
        "# of size N\u003d8 in random order, using two background proceses\n",
        "cifar10_train_dl \u003d torch.utils.data.DataLoader(\n",
        "    cifar10_train_ds, batch_size\u003d8, shuffle\u003dTrue, num_workers\u003d2\n",
        ")\n",
        "\n",
        "# Iterate over batches sampled with our DataLoader\n",
        "num_batches_to_show \u003d 5\n",
        "for idx, (images, classes) in enumerate(cifar10_train_dl):\n",
        "    # The DataLoader returns a tuple of:\n",
        "    # images: Tensor of size NxCxWxH\n",
        "    # classes: Tensor of size N\n",
        "    fig, axes \u003d plot.tensors_as_images(images, figsize\u003d(8, 1))\n",
        "    fig.suptitle(f\u0027Batch #{idx+1}:\u0027, x\u003d0, y\u003d0.6)\n",
        "    if idx \u003e\u003d num_batches_to_show - 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Training, Validation and Test Sets\n",
        "\u003ca id\u003dpart1_4\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now that we know about `DataLoaders` we can use them to do something useful: split a training dataset into **Training and Validation** sets.\n",
        "\n",
        "A common issue with machine learning models is abundance of hyperparameters that must be selected prior to training the model on data. More generally, multiple different models, or hypothesis classes could be fitted to the data.\n",
        "We would like to determine which model and/or hyperparameter selection of the same model can best fit the training data we have.\n",
        "\n",
        "How are such hyperparameters selected? How should their fitness be evaluated?\n",
        "\n",
        "While tempting, we can\u0027t use our test dataset to determine this. Doing so would be effectively equivalent to training with the test set, and may significantly bias our model towards overfitting, reducing it\u0027s generalization ability.\n",
        "\n",
        "A prevalent approach is therefore to split the training dataset into two parts:\n",
        "One for actual training, i.e. tuning model parameters e.g. weights in the case of neural nets,\n",
        "and another for validation, i.e. comparing one model or set of hyperparameters to another.\n",
        "After the best model is selected (by seeking the minimal validation error), it can be retrained with the entire training set.\n",
        "\n",
        "Crucially, test set performance is only evaluated once, at the end, after the best model has been selected and trained on the full training set. This provides us with an unbiased estimate of how our model will generalize to previously-unseen data.\n",
        "\n",
        "![img](https://cdn-images-1.medium.com/max/1600/1*Nv2NNALuokZEcV6hYEHdGA.png)\n",
        "\n",
        "**TODO** Implement the function `create_train_validation_loaders` in the `hw1/dataloaders.py` module.\n",
        "Use the following code block to check your implementation. Hint: you can specify a sampler class for the `DataLoader` instance you create."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# Testing the train/validation split dataloaders\n",
        "import hw1.dataloaders as hw1dataloaders\n",
        "\n",
        "validation_ratio \u003d 0.2\n",
        "dl_train, dl_valid \u003d hw1dataloaders.create_train_validation_loaders(cifar10_train_ds, validation_ratio)\n",
        "\n",
        "train_idx \u003d set(dl_train.sampler.indices)\n",
        "valid_idx \u003d set(dl_valid.sampler.indices)\n",
        "train_size \u003d len(train_idx)\n",
        "valid_size \u003d len(valid_idx)\n",
        "print(\u0027Training set size: \u0027, train_size)\n",
        "print(\u0027Validation set size: \u0027, valid_size)\n",
        "\n",
        "# Tests\n",
        "test.assertEqual(train_size+valid_size, len(cifar10_train_ds), \"Incorrect total number of samples\")\n",
        "test.assertEqual(valid_size, validation_ratio * (train_size + valid_size), \"Incorrect ratio\")\n",
        "test.assertTrue(train_idx.isdisjoint(valid_idx), \"Train and validation sets are not disjoint\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}